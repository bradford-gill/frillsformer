from layers import AddNorm, FeedForward, MultiHeadAttention
import torch.nn as nn

class Model(nn.Module):
    def __init__(self):
        pass
    
    def forward(self):
        pass